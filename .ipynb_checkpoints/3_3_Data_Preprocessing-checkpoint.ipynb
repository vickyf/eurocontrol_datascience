{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Data pre-processing is a phase in data mining, where you filter out out-of-range values, impossible data combination, missing values, etc. from the gathered data. The most important part we need to do is data cleaning.\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "When exploring and visualizing we can often ignore missing values cause much of the packages used can deal with them properly (such as the plotting tools we presented earlier). However we must always check for them first and be aware of them. It would not be the first time wrong conclusions were made because one was not aware of all the missing values in their data.\n",
    "\n",
    "On the other hand most of the models we will use later to make predictions cannot handle missing values. So instead of just ignoring them and being aware of them, we have to deal with them in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/housing.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are missing values in any of the columns?\n",
    "df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in which column are these missing values?\n",
    "df.total_bedrooms.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know where the missing values are situated, we can do one of the following things to cope with these missing values (or features values).\n",
    "\n",
    "1. Get rid of the corresponding rows\n",
    "2. Get rid of the entire feature/attribute/variable\n",
    "3. Set the missing values to some value (e.g. mean or median of that feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1:\n",
    "df_option1 = df.dropna(subset=[\"total_bedrooms\"])\n",
    "print(\"original size:\" + str(df.shape[0]))\n",
    "print(\"new size:\" + str(df_option1.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2:\n",
    "df_option2 = df.drop(\"total_bedrooms\", axis=1)\n",
    "df_option2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 3:\n",
    "median = df[\"total_bedrooms\"].median()\n",
    "df_option3 = df[\"total_bedrooms\"].fillna(median, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although at this point pandas is the most logical and convenient way to deal with missing values, `sklearn` also offers a standard way to deal with missing values. We will touch upon this because we will be making extensive use of `sklearn` later on as the primary package to fit and test models. More specifically  we will be making use of the `imputer` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE imputer only works on numerical features!\n",
    "df_num = df.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# and lets fit/setup the imputer\n",
    "imputer.fit(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and finally transform/apply this imputer\n",
    "X = imputer.transform(df_num)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most often this will then be used in a pipeline to run models on. Preprocessing however does not stop here. Depending on what models we use, or on what de data looks like, tranformations are in order.  For example when using a Naive Bayes model it is neccesary  that each feature has the same distribution, i.e. the data is standardized. Below we show how we can do all such transformations using `sklearn` preprocessing in a way similar to how we `imputed` the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxScaler\n",
    "To **rescale features to a given range**, the preprocessing packge provides `MinMaxScaler(feature_range=(0,1),copy=True)`. You can give it a **minimum** and a **maximum** with the featured range. The default is (0,1). You can also set copy to False if you want to change the array itself, rather than making a copy of it and rescaling this.\n",
    "\n",
    "It uses the following algorithm:\n",
    "\n",
    "    X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "where min,max = feature_range.\n",
    "\n",
    "Assume x an array we want to rescale. First we must **initiate a new scaler**, then **fit it to x** and finally **transform x with the scaler**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "    \n",
    "# lets take one feature from our dataframe:\n",
    "x = df.median_income.values.reshape(-1, 1)\n",
    "print(x[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want to rescale the values of this numpy array to the range from -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((-1,1))\n",
    "scaler.fit(x)\n",
    "scaled_x = scaler.transform(x)\n",
    "print(scaled_x[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting and transforming can be combined into a single command `fit_transform()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Plot a histogram of the following three normal distributions. Then rescale them using the MinMaxScaler to an interval of (0,1). Plot these as well and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1=20\n",
    "sigma1=10\n",
    "\n",
    "mu2=5\n",
    "sigma2=3\n",
    "\n",
    "mu3=0\n",
    "sigma3=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 3_Exploratory_Data_Analysis/minmaxscaler.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer\n",
    "The `normalizer` will rescale features so that each feature has a **unit norm**. It has a similar syntax to the MinMaxScaler.\n",
    "\n",
    "Unit norm essentially means that if we squared each element in the vector, and summed them, it would equal 1 (note this normalization is also often referred to as, unit norm or a vector of length 1 or a unit vector).\n",
    "\n",
    "Scaling inputs to unit norms is a common operation for text classification or clustering for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Normalize the following array. Check if the norm of each row is indeed 1. \n",
    "\n",
    "Use `np.isclose(a,b)` to check if two elements are equal within a tolerance. Use the `norm()` function from `np.linalg` to calculate the norm.\n",
    "\n",
    "https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.isclose.html\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 3_Exploratory_Data_Analysis/normalizer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler\n",
    "The standard scaler will **remove the mean** and **scale to unit variance**. This way the data is **centered around 0 and has variance 1**. It has a similar syntax to the other scalers.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Standardize the given normal distribution. Print the first 10 items of each distribution and check whether the mean and variance indeed approach 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.random.normal(6,4,(1000000,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 5_Machine_Learning/standardscaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats as s\n",
    "\n",
    "X = np.random.normal(6,4,(1000000,1))\n",
    "scaler = StandardScaler()\n",
    "standardized_X = scaler.fit_transform(X)\n",
    "print(X[0:10,:])\n",
    "print()\n",
    "print(standardized_X[0:10,:])\n",
    "print()\n",
    "print(np.isclose(s.describe(standardized_X).mean,0))\n",
    "print()\n",
    "print(np.isclose(s.describe(standardized_X).variance,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.hist(X)\n",
    "plt.hist(standardized_X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LabelEncoder and OneHotEncoder\n",
    "LabelEncoder will take an input of labels and **encode these as sequential integers**. It uses the same syntax as the scalers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "values = ['Warm', 'Cold', 'Warm', 'Hot', 'Hot', 'Cold']\n",
    "labenc = LabelEncoder()\n",
    "int_encoded = labenc.fit_transform(values)\n",
    "print(int_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot enconding uses a **series of bits**. Of these bits only one can be \"hot\" (1), all the others must be \"cold\" (0). All states with more than one hot bit are illegal. \n",
    "\n",
    "This kind of encoding is needed when **feeding categorical data to many scikit-learn estimators**, notably linear models and SVMs with the standard kernels. This way we're **not implying that certain categories have a higher rank than others**, like we are with LabelEncoders. It has a similar syntax to the other scalers we've discussed, but you need to **start from the label-encoded array**, not from the array with categories. \n",
    "\n",
    "Note: a one-hot encoding of y labels should use a LabelBinarizer instead.\n",
    "\n",
    "Here's an example of how to encode a simple list of categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "OHenc = OneHotEncoder()\n",
    "int_encoded = int_encoded.reshape(len(int_encoded),1)\n",
    "print(int_encoded)\n",
    "print()\n",
    "OH_encoded = OHenc.fit_transform(int_encoded).toarray()\n",
    "print(OH_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Make this table into a One hot encoded array. Compare the output from the label encoder to the one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = ['cat', 'dog', 'cat', 'cat','guinea pig','dog','elephant','cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 3_Exploratory_Data_Analysis/labelandonehotencoding.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy feature\n",
    "A dummy feature will add a feature to your data set that has the same value everywhere. It will be added as the first column of the data set. `sk.preprocessing.add_dummy_feature(X,value=1.0)`\n",
    "\n",
    "This is useful for fitting an intercept term with implementations which cannot otherwise fit it directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "Add a dummy feature of 7.5 to the given array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1., 4.5],[10.4, 7.2],[2.4, 9.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 3_Exploratory_Data_Analysis/dummyfeature.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
