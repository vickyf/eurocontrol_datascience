{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Curse of Dimensionality\n",
    "\n",
    "Up to now we have dealt with very manageable datasets. Often the problem in machine learning is not the amount of rows (items), but the number of features. Namely as the number of features starts to grow (I am talking millions here), training will become harder and slower. Moreover, as the number features grows, the number of data needed for proper training of any algorithm grows exponentially. This is what we call the curse of dimensionality.\n",
    "\n",
    "Luckily there are some ways to reduce the dimensionality of the data. Often features are correlated to some degree and they can be merged into some novel feature (hence one sometimes refers to this as feature extraction). To this end a first visualization of the multidimensional data as projected upon a visualizable amount of dimensions (preferably 2 of course), is a great first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection\n",
    "\n",
    "More particularly we will look at **manifold learning**. That is a 2D manifold is a 2D shape that can be bent and twisted in a higher dimensional space. So manifold learning is a way to \"extract\" this 2D shape. This might seem a bit abstract so lets go to an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next line to silence pyflakes. This import is needed.\n",
    "Axes3D\n",
    "\n",
    "n_points = 1000\n",
    "X, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\n",
    "n_neighbors = 10\n",
    "n_components = 2\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
    "             % (1000, n_neighbors), fontsize=14)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.view_init(4, -72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this actually is a 2D plane that is bend in the shape of an S in 3D space. Let's take a look at how we can entangle it back to 2D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ISOMAP algorithm for manifold learning\n",
    "Y = manifold.Isomap(n_neighbors, n_components).fit_transform(X)\n",
    "\n",
    "# n_neighbors: nearest neighbor algorithm based so how many neighbors?\n",
    "# n_components: output dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the transformation:\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other many other manifold learning algorithms exist. See what happens when you use the current very popular t-SNE algorithm: http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "Principal component analysis is a procedure that will **attempt to convert a set of observations of possibly correlated variables into a set of linearly uncorrelated variables**. These linearly unrelated variables will then be the principal components. The first principal component will then have the largest possible variance and thus **account for as much of the variability in the data as possible**. Every other component also has the largest possible variance under the constraint that they must be **orthogonal to the preceding components**.\n",
    "\n",
    "PCA can supply a **lower-dimensional picture**, because it tries to **condense the information as much as possible**. If we ommit the component with the lowest variance, the component that carries the least meaning, we retain as much information as possible in as little components as possible.\n",
    "\n",
    "Scikit learn implements this algorithm in their `sklearn.decomposition.PCA()` class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "We will look at the PCA using the **digits** dataset, cause it has many dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data #get features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before PCA it is often a good idea to standardize the data so that the length of each feature vector equals 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 5_Advanced/pca_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do a PCA, reducing the dataset to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original dimensionality:\" + str(X.shape[1]))\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "print(\"new dimensionality:\" + str(X2D.shape[1]))\n",
    "\n",
    "plt.scatter(X2D[:,0],X2D[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we got 2 dimensions. But is this a good data reduction? How do we know how many dimensions to choose? We do this using the explained variance ratio. That is, each component we add to the PCA will explain a small piece of the variance present in the data. As we add more components more variance will get explained until eventually all of the variance is explained when the number of components equals the number of variables. Try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load 5_Advanced/pca_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One traditional way to select how many components to select is by looking at the elbow in the graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
